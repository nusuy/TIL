> Review
>
> - 경사 하강법: 최적화를 위한 방향 판단을 위해 미분 (기울기 감소 방향으로 파라미터 조정)

# 로지스틱 회귀 Logistic Regression

- 이진 분류 알고리즘
  - 최종적으로 분류하는 것이 목적

> 선형회귀
>
> - 가설 중에서 데이터를 가장 잘 설명하는 직선 하나를 찾아야 하는 문제

## 선형 회귀 모델의 문제점

- 종속 변수가 범주형이라면 선형 회귀로 해결 가능한가?
- 출력이 -INF ~ INF이어서 확률값 0~1과 불일치
- 직선 -> Sigmoid 함수로 바꾸는 과정
  - 확률을 Threshold 기준으로 0과 1로 분류 (보통 0.5)
  - 계수 값에 따라 시그모이드 형태 결정
  - 경사 하강법 사용하기 어려운 형태
- **비선형**
- 오차: (정답 - 가설)^2

## 손실함수는 Mean Squared Error?

- 경사 하강법 사용 위해 MSE가 아닌 다른 손실함수로 변경
- Cross Entropy: 두 확률분포 사이의 오차 측정
  - 두 개의 그래프 합친 Loss 함수 (손실 함수)

# 딥러닝

### 배치 경사 하강법

- 배치 경사 하강법
- 확률적 경사 하강법
- 미니 배치 경사 하강법

### 다중 선형회귀

> vs. 다항 선형회귀
>
> - 다중: 여러 개의 독립 변수가 종속 변수에 영향 (선형 모델)
> - 다항: 한 독립 변수에 따른 종속 변수와의 관계가 다항식 (선형 모델)

## 이진 분류

### Sigmoid 함수

- 활성화 함수 activation function

## 다중 분류

### Softmax 사용

- 다중 클래스 분류일 경우, Sigmoid 대신 Softmax 함수 사용
- 비용 함수 변경 필요 Categorical Cross Entropy

## 정리

|           | 활성화 함수 |         손실 함수         |   최적화 방법    |
| :-------: | :---------: | :-----------------------: | :--------------: |
| 선형 회귀 |      -      |    Mean Squared Error     | Gradient Descent |
| 2진 분류  |   Sigmoid   |   Binary Cross Entropy    | Gradient Descent |
| 다중 분류 |   Softmax   | Categorical Cross Entropy | Gradient Descent |

# 뉴럴 네트워크 Neural Network

## 퍼셉트론 (Perceptron)

- Activation function: 다음 단계로 신호를 전달할지 말지 결정

### XOR 문제

- 레이어 증가하면 비선형 성질 증가

## 오차 역전파 Backpropagation

- MLP 학습 알고리즘
- 오차(손실)을 네트워크의 뒤로 전달하며 Weight을 갱신시키는 과정

### 역전파

- 임의의 한 노드에서의 미분값은 바로 앞 노드에서 넘어오는 미분값에 로컬 미분값을 곱하는 것만으로 계산 가능

## 기울기 소실 Vanishing Gradient

- 깊은 신경망의 경우, 입력층으로 갈수록 역전파 과정에서 미분값이 사라지면서 학습이 안 되는 현상

### 원인

- 역전파 과정에서 Sigmoid의 미분값이 연속적으로 곱해지며 기울기가 0으로 사라짐
- 출력층은 무조건 Sigmoid
- ReLU

### Optimizers

- 경사하강법

## 교차 검증

- 딥러닝에서는 교차검증 X
- 데이터가 너무 커서

## 데이터 증강 Data Augmentation

- 데이터 양이 충분하지 않으면 과적합 발생
- 기존 데이터 변경 / 생성 모델로 생성
- 이미지 변형 후 미리 데이터셋에 주가할 수도 있지만, 학습 과정에서 모델에 입력하기 전 실시간으로 증강해서 사용

## Dropout 과적합 방지 (규제)

- 모델 학습 시에만 적용
- 도델의 다양성을 위한 방법 (앙상블)

## 가중치 초기화

- Weight과 Bias를 어떻게 설정

## 배치 정규화 Batch Normalization

- 가중치 초기값에 상관없이 활성화값(활성화 함수 출력값) 강제 분포
- Affine Layer를 통과한 미니배치 단위 Output을 표준정규분포로 정규화
- overfitting 억제

## Early Stopping

- 학습 조기 종료
